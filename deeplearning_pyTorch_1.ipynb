{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_pyTorch_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoTYFFmog+N0rqIUXzm0s8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhinoos-adibi/Computer_Vision/blob/main/deeplearning_pyTorch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqgb6skP5Pzy",
        "outputId": "f39be9e9-6120-4dca-d7df-5cc67136d75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4793, 0.2684, 0.9696],\n",
            "        [0.9020, 0.7549, 0.5082],\n",
            "        [0.6903, 0.2196, 0.0474]])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "#The refrece of these codes is DataCamp :Course name in DataCamp is :Introduction Deepleaning with pyTorch\n",
        "# in this codes we will create tensor with torch\n",
        "\n",
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Create random tensor of size 3 by 3\n",
        "your_first_tensor = torch.rand(3, 3)\n",
        "\n",
        "# Calculate the shape of the tensor\n",
        "tensor_size = your_first_tensor.shape\n",
        "\n",
        "# Print the values of the tensor and its shape\n",
        "print(your_first_tensor)\n",
        "print(tensor_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In this block of codes we can see difference between torch.matmul multiplication and \n",
        "# element-wise multiplication\n",
        "\n",
        "# Create a matrix of ones with shape 3 by 3\n",
        "tensor_of_ones = torch.ones(3, 3)\n",
        "\n",
        "# Create an identity matrix with shape 3 by 3\n",
        "identity_tensor = torch.eye(3)\n",
        "\n",
        "# Do a matrix multiplication of tensor_of_ones with identity_tensor\n",
        "matrices_multiplied = torch.matmul(tensor_of_ones, identity_tensor)\n",
        "print(\"matrices_multiplied\\n\",matrices_multiplied,\"\\n\\n\")\n",
        "\n",
        "# Do an element-wise multiplication of tensor_of_ones with identity_tensor\n",
        "element_multiplication = tensor_of_ones * identity_tensor\n",
        "print(\"element_multiplication\\n\",element_multiplication)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWZn7tzV77T4",
        "outputId": "ced82596-fa82-4c02-f9b2-c59320ce1430"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrices_multiplied\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "\n",
            "element_multiplication\n",
            " tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "# Let's have something resembling more a neural network. The computational graph has been given below.\n",
        "#  You are going to initialize 3 large random tensors, and then do the operations\n",
        "#   as given in the computational graph. \n",
        "# The final operation is the mean of the tensor, given by torch.mean(your_tensor)."
      ],
      "metadata": {
        "id": "7Orj8oaJ_rHT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tensors x, y and z\n",
        "x = torch.rand(1000, 1000)\n",
        "y = torch.rand(1000, 1000)\n",
        "z = torch.rand(1000, 1000)\n",
        "\n",
        "# Multiply x with y\n",
        "q = torch.matmul(x, y)\n",
        "\n",
        "# Multiply elementwise z with q\n",
        "f = z * q\n",
        "\n",
        "mean_f = torch.mean(f)\n",
        "print(mean_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683BM0wAFuFM",
        "outputId": "62bfe975-d9ca-444e-b922-4f1e55a56917"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(124.9761)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize x, y and z to values 4, -3 and 5\n",
        "x = torch.tensor(4., requires_grad=True)\n",
        "y = torch.tensor(-3., requires_grad=True)\n",
        "z = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "# Set q to sum of x and y, set f to product of q with z\n",
        "q = x + y\n",
        "f = q * z\n",
        "\n",
        "# Compute the derivatives\n",
        "f.backward()\n",
        "\n",
        "# Print the gradients\n",
        "print(\"Gradient of x is: \" + str(x.grad))\n",
        "print(\"Gradient of y is: \" + str(y.grad))\n",
        "print(\"Gradient of z is: \" + str(z.grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsIT0sA_Gbh5",
        "outputId": "2f17e959-0b7b-4090-8035-5f8f2f75858c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x is: tensor(5.)\n",
            "Gradient of y is: tensor(5.)\n",
            "Gradient of z is: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You are going to build a neural network in PyTorch, using the hard way. \n",
        "# Your input will be images of size (28, 28), so images containing 784 pixels.\n",
        "# Your network will contain an input_layer (provided for you), a hidden layer with 200 units, \n",
        "# and an output layer with 10 classes. The input layer has already been created for you. \n",
        "# You are going to create the weights, \n",
        "# and then do matrix multiplications, getting the results from the network."
      ],
      "metadata": {
        "id": "EdpSQPw5ssLg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "input_layer = torch.tensor([4.3188e-01, 4.7354e-01, 2.6275e-01, 6.3537e-01, 9.8212e-01, 2.6415e-01,\n",
        "        6.4142e-01, 2.3079e-01, 1.3732e-01, 5.3356e-01, 7.2748e-01, 9.4434e-01,\n",
        "        5.7105e-01, 9.9881e-01, 9.0072e-01, 5.6095e-02, 7.9118e-01, 3.4006e-01,\n",
        "        3.0109e-01, 2.9298e-01, 4.3131e-01, 2.3506e-01, 8.2543e-01, 9.9515e-02,\n",
        "        2.8579e-01, 7.2605e-01, 6.0253e-01, 3.9238e-01, 8.1135e-01, 2.4731e-01,\n",
        "        7.4149e-01, 9.3424e-01, 3.7170e-01, 4.3497e-01, 2.8626e-02, 4.9048e-01,\n",
        "        5.6343e-01, 7.0524e-01, 1.4156e-01, 5.3479e-01, 8.2444e-01, 2.4634e-01,\n",
        "        7.2249e-01, 3.9963e-01, 5.0056e-01, 2.8583e-01, 1.8644e-01, 8.2895e-01,\n",
        "        9.0891e-01, 6.0592e-01, 2.7220e-01, 7.2300e-01, 7.5743e-01, 6.3799e-01,\n",
        "        8.0192e-01, 4.0917e-02, 4.1576e-01, 4.0570e-01, 1.6425e-01, 5.4499e-02,\n",
        "        5.0476e-01, 1.0052e-01, 8.1795e-01, 9.1300e-01, 4.5272e-01, 6.0074e-01,\n",
        "        1.7778e-01, 6.4347e-01, 2.2719e-01, 1.8113e-01, 9.5389e-01, 1.0228e-01,\n",
        "        4.5161e-01, 1.3401e-01, 5.5488e-01, 5.6030e-01, 2.0242e-01, 1.1947e-01,\n",
        "        6.0880e-01, 2.7578e-01, 9.7915e-01, 4.8281e-01, 1.8302e-03, 9.3875e-01,\n",
        "        4.1642e-01, 8.5645e-01, 6.8642e-01, 3.6299e-01, 2.9879e-01, 6.2237e-02,\n",
        "        1.9751e-01, 4.1013e-01, 1.9702e-01, 9.1698e-01, 5.4804e-02, 5.3635e-01,\n",
        "        2.2906e-01, 1.3667e-03, 3.1929e-01, 8.3109e-01, 7.4755e-01, 3.9086e-01,\n",
        "        1.5527e-01, 7.4859e-01, 3.1165e-01, 2.0864e-02, 1.9849e-01, 6.9502e-01,\n",
        "        6.3329e-01, 6.5560e-03, 2.4800e-02, 7.7928e-01, 3.5393e-01, 1.5955e-01,\n",
        "        3.9996e-01, 3.3061e-01, 5.4684e-01, 9.8438e-01, 6.1419e-01, 1.7630e-01,\n",
        "        5.7273e-01, 3.5734e-01, 7.9285e-01, 7.2755e-01, 6.0702e-01, 5.3423e-01,\n",
        "        3.7331e-02, 6.2052e-02, 4.2179e-01, 9.7493e-01, 8.2826e-01, 7.3977e-01,\n",
        "        8.7810e-01, 5.0109e-01, 7.8546e-01, 1.2122e-01, 5.6360e-01, 7.4938e-01,\n",
        "        6.1675e-01, 5.9191e-02, 1.8274e-01, 3.6918e-01, 6.3803e-03, 5.5539e-01,\n",
        "        2.4929e-01, 7.9612e-01, 6.2434e-01, 6.2358e-01, 3.1554e-01, 4.7957e-01,\n",
        "        5.7906e-01, 1.1085e-01, 1.8439e-01, 7.4043e-02, 5.9787e-01, 4.6514e-01,\n",
        "        3.3958e-01, 5.3031e-01, 4.6787e-02, 3.8065e-01, 8.5221e-01, 5.9887e-01,\n",
        "        8.4250e-01, 2.6112e-01, 7.9896e-01, 3.4970e-01, 9.3121e-01, 6.0417e-02,\n",
        "        2.5488e-01, 3.9866e-01, 7.5416e-01, 3.0484e-02, 2.4502e-01, 5.4245e-01,\n",
        "        6.5317e-01, 6.5702e-01, 3.1985e-01, 3.5146e-01, 3.7694e-02, 6.1478e-01,\n",
        "        5.9586e-01, 5.4349e-01, 5.6727e-01, 5.2889e-01, 1.2050e-01, 7.5592e-01,\n",
        "        4.2072e-03, 2.0250e-01, 5.1780e-01, 3.6988e-02, 9.0342e-01, 4.5216e-01,\n",
        "        9.5839e-01, 5.9720e-01, 6.9858e-01, 6.4451e-02, 2.1254e-01, 3.6522e-02,\n",
        "        2.1782e-01, 6.9438e-01, 3.7880e-03, 1.0655e-02, 8.4945e-01, 2.2303e-01,\n",
        "        7.5845e-01, 3.9302e-01, 6.9185e-01, 5.8215e-01, 8.6565e-01, 6.4960e-01,\n",
        "        7.4849e-01, 4.2108e-01, 6.2536e-01, 9.9258e-02, 5.6519e-02, 2.4366e-01,\n",
        "        3.1804e-01, 4.3362e-01, 7.1429e-01, 7.0917e-01, 3.0320e-01, 8.6080e-01,\n",
        "        5.1212e-01, 4.2749e-02, 9.8719e-01, 1.4375e-01, 3.6246e-01, 8.2838e-01,\n",
        "        9.0129e-01, 7.6891e-01, 9.7165e-03, 5.9447e-01, 2.7325e-01, 5.5367e-01,\n",
        "        7.4356e-01, 2.1242e-01, 3.3427e-03, 2.5389e-02, 6.9992e-01, 5.5903e-01,\n",
        "        5.1483e-01, 5.9365e-01, 5.1960e-01, 8.7989e-01, 6.1713e-01, 7.8778e-01,\n",
        "        5.2291e-01, 4.7302e-01, 9.0149e-01, 3.1098e-01, 2.0556e-01, 9.2454e-02,\n",
        "        7.4431e-01, 4.9177e-01, 5.5967e-01, 8.7166e-01, 4.9822e-01, 9.5544e-01,\n",
        "        1.7125e-01, 9.5282e-01, 6.1263e-01, 1.7522e-01, 8.0559e-01, 5.3639e-01,\n",
        "        4.6456e-04, 8.0395e-01, 8.7011e-01, 8.6912e-01, 2.5949e-01, 9.6275e-01,\n",
        "        4.5085e-01, 3.6169e-01, 3.5958e-01, 2.2512e-01, 7.9295e-01, 1.0372e-01,\n",
        "        6.0968e-01, 1.1769e-03, 3.4345e-01, 4.5019e-01, 2.5660e-01, 3.6982e-01,\n",
        "        8.8831e-01, 3.4418e-03, 7.2134e-01, 8.2083e-01, 5.6434e-01, 4.1827e-01,\n",
        "        3.7564e-01, 5.2654e-01, 5.4542e-01, 3.6698e-01, 3.5088e-01, 3.7449e-01,\n",
        "        4.9271e-02, 5.6510e-01, 9.1426e-01, 3.5256e-01, 3.1063e-01, 8.5184e-01,\n",
        "        8.4801e-01, 8.8895e-01, 4.4508e-02, 7.5639e-01, 5.7951e-01, 6.2675e-01,\n",
        "        7.9519e-01, 2.3797e-02, 2.9171e-01, 7.1647e-01, 3.6869e-03, 6.4349e-02,\n",
        "        8.6814e-01, 6.8707e-01, 5.0414e-01, 7.2193e-01, 1.7596e-01, 1.0326e-01,\n",
        "        2.9923e-01, 9.2986e-01, 4.7254e-01, 1.6040e-01, 8.2591e-01, 7.1106e-01,\n",
        "        1.8748e-01, 1.2189e-01, 9.9480e-01, 6.3312e-02, 1.9889e-01, 6.7948e-01,\n",
        "        2.9159e-01, 5.9445e-01, 5.9575e-01, 4.9756e-01, 1.3093e-01, 1.2854e-01,\n",
        "        7.2303e-01, 8.2480e-01, 7.1350e-01, 8.8748e-01, 6.5394e-01, 7.5260e-01,\n",
        "        1.0650e-01, 6.1355e-01, 3.6894e-01, 1.9434e-01, 2.7772e-01, 5.3961e-01,\n",
        "        9.9105e-01, 5.6505e-01, 3.2450e-01, 7.7126e-01, 9.6105e-01, 5.0865e-01,\n",
        "        4.7838e-01, 7.3675e-02, 2.5669e-02, 9.2736e-01, 7.6094e-01, 6.7920e-01,\n",
        "        6.4675e-01, 7.7819e-01, 6.1931e-01, 6.7357e-02, 5.6139e-01, 7.5399e-01,\n",
        "        9.2975e-01, 6.2153e-01, 1.3242e-01, 4.6293e-01, 5.5902e-01, 2.9089e-01,\n",
        "        5.1504e-01, 8.8928e-01, 9.9862e-02, 1.1880e-01, 3.4950e-02, 3.1122e-02,\n",
        "        2.9244e-01, 3.0711e-01, 1.6917e-01, 9.0897e-01, 5.3853e-01, 4.3224e-03,\n",
        "        2.8356e-01, 5.7836e-01, 4.9660e-01, 8.0994e-01, 2.5150e-01, 7.6524e-01,\n",
        "        8.6823e-01, 9.2523e-01, 4.4922e-01, 7.1325e-01, 6.3613e-01, 3.2740e-01,\n",
        "        7.1817e-01, 7.9057e-01, 9.1434e-01, 5.3668e-02, 1.4698e-01, 6.9929e-01,\n",
        "        7.9837e-01, 9.2991e-01, 2.2725e-01, 8.3177e-01, 7.8429e-01, 1.5230e-01,\n",
        "        5.1664e-01, 9.6270e-01, 9.7282e-01, 1.0623e-01, 7.1939e-01, 4.3890e-01,\n",
        "        7.1575e-01, 2.3721e-01, 4.1746e-01, 2.0440e-01, 8.6840e-01, 7.0141e-01,\n",
        "        8.9190e-01, 9.5982e-01, 4.3335e-01, 9.5951e-01, 8.5354e-01, 9.5662e-01,\n",
        "        4.6494e-01, 3.6168e-01, 1.7764e-01, 8.2848e-02, 7.3066e-01, 1.8532e-01,\n",
        "        7.6441e-02, 5.4328e-02, 2.2424e-01, 7.9908e-01, 3.8878e-01, 1.5990e-01,\n",
        "        7.8067e-01, 7.2444e-02, 8.3045e-01, 1.6578e-01, 8.6147e-01, 3.8233e-01,\n",
        "        7.6253e-01, 8.3672e-02, 4.3988e-01, 4.2983e-01, 1.2050e-01, 4.4863e-01,\n",
        "        3.4506e-01, 8.1194e-01, 8.1412e-01, 3.7039e-01, 2.7258e-01, 3.3258e-01,\n",
        "        9.1702e-01, 7.6137e-03, 6.7593e-01, 2.9603e-01, 7.9642e-01, 8.8536e-01,\n",
        "        8.3529e-01, 2.9833e-01, 8.5341e-02, 1.5288e-01, 6.7862e-01, 7.4439e-01,\n",
        "        9.9541e-01, 2.3504e-02, 4.1884e-02, 4.8695e-01, 7.4820e-01, 4.7144e-01,\n",
        "        4.1082e-01, 9.6981e-01, 1.0533e-01, 2.9186e-01, 4.2388e-01, 9.9908e-01,\n",
        "        2.9095e-01, 6.7937e-01, 2.6896e-01, 1.3948e-01, 2.0193e-01, 7.0425e-01,\n",
        "        4.8068e-01, 9.3720e-01, 9.4881e-01, 2.6870e-01, 7.6178e-01, 4.9190e-01,\n",
        "        8.8075e-01, 6.6854e-03, 6.4823e-01, 3.7097e-01, 8.9322e-01, 5.3263e-01,\n",
        "        3.6141e-02, 5.7764e-01, 9.5248e-01, 3.9058e-01, 7.5809e-01, 2.2863e-01,\n",
        "        7.9746e-02, 8.3289e-01, 2.1737e-01, 8.5430e-01, 2.8305e-01, 2.4173e-01,\n",
        "        6.8327e-01, 5.2630e-01, 6.0529e-01, 1.3457e-02, 2.0503e-02, 1.7561e-01,\n",
        "        7.7002e-01, 7.0433e-01, 9.6165e-02, 3.3254e-01, 4.7760e-01, 3.4823e-01,\n",
        "        8.6677e-01, 3.5675e-01, 5.3246e-01, 8.4586e-02, 6.9451e-01, 2.9185e-01,\n",
        "        7.0764e-01, 4.8238e-01, 6.7481e-01, 3.2368e-01, 7.9449e-01, 2.2693e-01,\n",
        "        1.0300e-01, 6.0251e-01, 8.1334e-01, 6.3225e-01, 5.0836e-01, 1.4402e-01,\n",
        "        9.3585e-01, 6.2241e-01, 8.9938e-01, 8.7730e-01, 6.6806e-01, 5.3816e-02,\n",
        "        6.9654e-01, 6.3285e-01, 6.3627e-01, 4.0286e-01, 5.2921e-01, 9.4531e-01,\n",
        "        2.9381e-01, 7.2799e-01, 1.9100e-01, 2.2802e-01, 7.8151e-01, 9.8903e-01,\n",
        "        3.4696e-01, 2.2187e-02, 9.8452e-01, 8.7519e-01, 2.7261e-01, 5.9981e-01,\n",
        "        4.6098e-01, 2.3617e-01, 9.3277e-01, 5.9113e-01, 6.6411e-01, 1.7672e-01,\n",
        "        2.7119e-01, 9.7045e-01, 5.6832e-02, 4.1798e-02, 1.0501e-01, 7.5420e-02,\n",
        "        7.0539e-01, 7.3473e-01, 2.7905e-01, 5.4338e-01, 8.7034e-01, 7.7910e-01,\n",
        "        9.8531e-01, 4.4623e-01, 2.5641e-03, 4.7024e-01, 8.8611e-01, 8.2014e-01,\n",
        "        4.6527e-01, 9.0805e-02, 9.5582e-01, 6.0475e-01, 6.9652e-01, 5.5953e-01,\n",
        "        1.2686e-01, 9.3996e-01, 4.8710e-02, 2.0255e-01, 6.3054e-01, 6.0981e-01,\n",
        "        7.4623e-01, 5.5870e-02, 8.4056e-01, 3.9050e-01, 8.1359e-02, 2.2594e-01,\n",
        "        1.4757e-01, 6.0043e-01, 1.2219e-01, 6.4391e-01, 5.3076e-01, 9.1271e-01,\n",
        "        4.8608e-01, 4.4502e-01, 2.1132e-01, 1.5696e-01, 2.8655e-01, 4.0168e-02,\n",
        "        4.3296e-01, 4.3639e-01, 4.3940e-01, 7.3032e-01, 5.1908e-01, 1.5206e-02,\n",
        "        2.7682e-01, 4.9689e-01, 5.9166e-01, 1.0515e-01, 4.5975e-01, 4.7617e-01,\n",
        "        9.6352e-01, 7.9714e-01, 5.8577e-01, 8.9944e-02, 7.6518e-01, 6.0593e-01,\n",
        "        6.6771e-01, 4.5905e-01, 5.1384e-01, 2.5539e-01, 9.4434e-01, 4.2499e-01,\n",
        "        4.4443e-01, 9.6015e-01, 5.1285e-01, 6.1355e-01, 5.5442e-02, 1.5376e-03,\n",
        "        3.9297e-01, 5.1674e-01, 8.2178e-01, 5.2060e-01, 8.0679e-01, 9.7602e-01,\n",
        "        7.7943e-01, 1.4669e-01, 4.6230e-01, 8.2672e-01, 7.0506e-01, 4.1501e-01,\n",
        "        4.3843e-01, 8.6933e-01, 8.6289e-01, 3.2543e-01, 7.5525e-01, 3.6251e-01,\n",
        "        9.5296e-01, 5.4597e-01, 6.9165e-01, 6.4292e-01, 4.4332e-01, 8.7371e-01,\n",
        "        8.1238e-02, 2.7867e-01, 2.2358e-01, 4.2342e-01, 7.5528e-01, 5.5049e-01,\n",
        "        2.2957e-01, 6.0286e-01, 7.6595e-01, 8.6787e-01, 1.7362e-01, 4.8372e-01,\n",
        "        7.6701e-01, 4.9559e-01, 9.7022e-01, 3.2475e-01, 2.0462e-02, 2.8365e-01,\n",
        "        9.6109e-01, 2.9338e-01, 6.7170e-01, 3.8557e-01, 1.8754e-01, 8.4174e-01,\n",
        "        4.4089e-02, 9.4808e-01, 2.8799e-01, 9.3624e-01, 1.1360e-01, 2.8190e-01,\n",
        "        4.5120e-01, 5.9909e-01, 5.3808e-01, 7.2183e-01, 8.1156e-01, 8.1343e-01,\n",
        "        9.3722e-01, 8.4983e-01, 4.6954e-01, 2.3638e-01, 8.5666e-01, 5.0234e-01,\n",
        "        8.5388e-01, 4.4799e-01, 4.4144e-02, 9.7617e-01, 8.3994e-01, 6.4761e-01,\n",
        "        1.2544e-01, 7.5641e-01, 8.4715e-01, 8.4777e-01, 9.7430e-01, 3.4395e-01,\n",
        "        5.5775e-01, 4.6992e-01, 4.1985e-01, 4.2576e-01, 7.9508e-01, 7.6021e-01,\n",
        "        6.9944e-01, 2.9396e-01, 2.1740e-01, 7.0135e-01, 7.2142e-01, 7.4261e-01,\n",
        "        1.9481e-01, 3.4777e-01, 6.8617e-01, 6.7233e-01, 6.2998e-01, 9.8056e-01,\n",
        "        8.1634e-01, 8.1203e-01, 8.1260e-01, 4.2735e-01, 6.1887e-01, 2.9095e-01,\n",
        "        6.7444e-02, 7.7022e-01, 1.2843e-01, 1.0301e-01, 5.5745e-01, 3.2516e-01,\n",
        "        5.5039e-01, 9.4231e-01, 7.7251e-01, 4.0811e-01, 5.4677e-01, 4.9574e-01,\n",
        "        5.3095e-01, 2.4050e-01, 3.4867e-02, 3.7269e-01, 1.9990e-01, 1.8291e-01,\n",
        "        1.8516e-01, 5.0409e-01, 4.8305e-01, 2.5608e-01, 9.9381e-01, 1.2747e-01,\n",
        "        4.0241e-01, 1.1974e-02, 3.2438e-01, 9.2349e-01, 4.9084e-01, 3.1929e-01,\n",
        "        4.5837e-01, 8.2032e-01, 1.3624e-01, 8.8436e-01])"
      ],
      "metadata": {
        "id": "ZpxiTzuv8Fds"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights of the neural network\n",
        "weight_1 = torch.rand(784, 200)\n",
        "weight_2 = torch.rand(200, 10)\n",
        "\n",
        "# Multiply input_layer with weight_1\n",
        "hidden_1 = torch.matmul(input_layer, weight_1)\n",
        "\n",
        "# Multiply hidden_1 with weight_2\n",
        "output_layer = torch.matmul(hidden_1, weight_2)\n",
        "print(output_layer)\n",
        "print(output_layer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r3qgc8I8EFd",
        "outputId": "dce47de4-e638-4324-e76d-48c770b535be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18612.1992, 19091.0449, 20268.7676, 18444.1992, 19181.5195, 19691.7793,\n",
            "        19453.5938, 19762.1152, 20797.7656, 19801.5195])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eUprjjIJ8kEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}